{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59dd5f97-041d-4347-8153-673274370473",
   "metadata": {
    "name": "description"
   },
   "source": [
    "### Data Engineering Pipelines with Snowflake Notebooks\n",
    "\n",
    "What You'll Learn\n",
    "How to ingest custom file formats (like Excel) with Snowpark from an external stage (such as an S3 bucket) into a Snowflake table\n",
    "How to access data from Snowflake Marketplace and use it for your analysis\n",
    "How to use Snowflake Notebooks and the Snowpark DataFrame API to build data engineering pipelines\n",
    "How to add logging to your Python data engineering code and monitor from within Snowsight\n",
    "How to execute SQL scripts from your Git repository directly in Snowflake\n",
    "How to use open-source Python libraries from curated Snowflake Anaconda channel\n",
    "How to use the Snowflake Python Management API to programmatically work with Snowflake objects\n",
    "How to use the Python Task DAG API to programatically manage Snowflake Tasks\n",
    "How to build CI/CD pipelines using Snowflake's Git Integration, the Snowflake CLI, and GitHub Actions\n",
    "How to deploy Snowflake Notebooks from dev to production\n",
    "\n",
    "Source Data:\n",
    "\n",
    "Tutorial: https://quickstarts.snowflake.com/guide/data_engineering_with_notebooks/index.html?index=..%2F..index#0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10781105-4998-4e2b-9549-09ff961b13ef",
   "metadata": {
    "language": "python",
    "name": "imports"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from datetime import timedelta\n",
    "\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "from snowflake.core import Root\n",
    "# import snowflake.snowpark.functions as F\n",
    "from snowflake.core.task.dagv1 import DAGOperation, DAG, DAGTask, DAGRun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672b22bd-4033-4f52-8f61-528085487cac",
   "metadata": {
    "language": "sql",
    "name": "set_ddl",
    "vscode": {
     "languageId": "snowflake-sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Set DDL\n",
    "\n",
    "-- Warehouses\n",
    "CREATE OR REPLACE WAREHOUSE TEST_WH WAREHOUSE_SIZE = XSMALL, AUTO_SUSPEND = 300, AUTO_RESUME= TRUE;\n",
    "USE WAREHOUSE TEST_WH;\n",
    "\n",
    "-- Databases\n",
    "CREATE OR ALTER DATABASE data_engineering_with_snowflake_notebooks;\n",
    "ALTER DATABASE data_engineering_with_snowflake_notebooks SET LOG_LEVEL = INFO;\n",
    "USE DATABASE data_engineering_with_snowflake_notebooks;\n",
    "\n",
    "-- Schemas\n",
    "CREATE OR REPLACE SCHEMA INTEGRATIONS;\n",
    "CREATE OR REPLACE SCHEMA DEV;\n",
    "\n",
    "-- Stages\n",
    "USE SCHEMA INTEGRATIONS;\n",
    "CREATE OR REPLACE STAGE FROSTBYTE_RAW_STAGE\n",
    "    URL = 's3://sfquickstarts/data-engineering-with-snowpark-python/';\n",
    "\n",
    "\n",
    "-- Events\n",
    "CREATE EVENT TABLE data_engineering_with_snowflake_notebooks.INTEGRATIONS.EVENTS;\n",
    "ALTER ACCOUNT SET EVENT_TABLE = data_engineering_with_snowflake_notebooks.INTEGRATIONS.EVENTS;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e525b189-9fd7-41dd-9548-5726f737540c",
   "metadata": {
    "language": "sql",
    "name": "set_github",
    "vscode": {
     "languageId": "snowflake-sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Set GitHub\n",
    "\n",
    "SET GITHUB_USERNAME = ; -- TODO!!!\n",
    "SET GITHUB_URL = 'https://github.com';\n",
    "SET GITHUB_REPO = 'util_lib';  -- repo holding the helper scripts\n",
    "SET GITHUB_URL_PREFIX = $GITHUB_URL || '/' || $GITHUB_USERNAME;\n",
    "SET GITHUB_REPO_ORIGIN =  $GITHUB_URL || '/' || $GITHUB_USERNAME || '/' || $GITHUB_REPO;\n",
    "\n",
    "-- Secrets (schema level)\n",
    "USE SCHEMA INTEGRATIONS;\n",
    "CREATE OR REPLACE SECRET GITHUB_SECRET\n",
    "  TYPE = password\n",
    "  USERNAME = $GITHUB_USERNAME\n",
    "  PASSWORD = ; -- TODO!!!\n",
    "\n",
    "-- API Integration (account level)\n",
    "CREATE OR REPLACE API INTEGRATION GITHUB_API_INTEGRATION\n",
    "  API_PROVIDER = GIT_HTTPS_API\n",
    "  API_ALLOWED_PREFIXES = ($GITHUB_URL_PREFIX)  --parenthesis required\n",
    "  ALLOWED_AUTHENTICATION_SECRETS = (GITHUB_SECRET)  --parenthesis required\n",
    "  ENABLED = TRUE;\n",
    "\n",
    "-- Git Repository\n",
    "CREATE OR REPLACE GIT REPOSITORY GIT_REPO\n",
    "  API_INTEGRATION = GITHUB_API_INTEGRATION\n",
    "  GIT_CREDENTIALS = GITHUB_SECRET\n",
    "  ORIGIN = $GITHUB_REPO_ORIGIN;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc7046e-f322-4832-8b1f-0b8c48cb7954",
   "metadata": {
    "name": "load_weather"
   },
   "source": [
    "### Load Weather\n",
    "\n",
    "Connect to the \"Weather Source LLC: frostbyte\" feed from Weather Source in the Snowflake Data Marketplace by following these steps:\n",
    "\n",
    "    -> Snowsight Home Button\n",
    "         -> Marketplace\n",
    "             -> Search: \"Weather Source LLC: frostbyte\" (and click on tile in results)\n",
    "                 -> Click the blue \"Get\" button\n",
    "                     -> Under \"Options\", adjust the Database name to read \"FROSTBYTE_WEATHERSOURCE\" (all capital letters)\n",
    "                        -> Grant to \"AccountAdmin\"\n",
    "    \n",
    "That's it... we don't have to do anything from here to keep this data updated.\n",
    "The provider will do that for us and data sharing means we are always seeing\n",
    "whatever they they have published."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387e1cb4-7b92-4fa8-b627-a9f3f98e9b76",
   "metadata": {
    "language": "sql",
    "name": "deploy_notebooks",
    "vscode": {
     "languageId": "snowflake-sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Deploy external notebooks\n",
    " \n",
    "\n",
    "EXECUTE IMMEDIATE FROM @GITHUB_REPO/branches/main/deploy_notebooks.sql\n",
    "    USING (\n",
    "        notebook => 'load_excel_files',\n",
    "        source_wh => 'TEST_WH',\n",
    "        source_db => 'data_engineering_with_snowflake_notebooks',\n",
    "        source_schema => 'INTEGRATIONS',\n",
    "        source_repo => 'data_engineering_with_snowflake_notebooks',\n",
    "        source_branch => 'DEV',\n",
    "        source_directory =>'notebooks',\n",
    "        target_db => 'data_engineering_with_snowflake_notebooks',\n",
    "        target_schema => 'DEV', \n",
    "        source_db => 'data_engineering_with_snowflake_notebooks'            \n",
    "        );\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bee7e93-a7c5-4065-b7be-a76bff862dae",
   "metadata": {
    "language": "python",
    "name": "cell9"
   },
   "outputs": [],
   "source": [
    "daily_city_metrics_upsert = \"\"\"\n",
    "\n",
    "import time\n",
    "from snowflake.snowpark import Session\n",
    "import snowflake.snowpark.functions as F\n",
    "\n",
    "# Define daily_city_metrics_upsert\n",
    "def main(session, schema):\n",
    "\n",
    "    table ='DAILY_CITY_METRICS'\n",
    "\n",
    "    # Temporarily increase warehouse size\n",
    "    session.sql(\"ALTER WAREHOUSE TEST_WH SET WAREHOUSE_SIZE = XLARGE WAIT_FOR_COMPLETION = TRUE\").collect()\n",
    " \n",
    "    \n",
    "    # Get data\n",
    "    # Define the tables\n",
    "    order_detail_df = session.table(\"ORDER_DETAIL\")\n",
    "    history_day_df = session.table(\"FROSTBYTE_WEATHERSOURCE.ONPOINT_ID.HISTORY_DAY\")\n",
    "    location_df = session.table(\"LOCATION\")\n",
    "\n",
    "    # Join the tables\n",
    "    daily_city_metrics_stg_df = order_detail_df.join(location_df, order_detail_df['LOCATION_ID'] == location_df['LOCATION_ID'])\n",
    "    daily_city_metrics_stg_df = daily_city_metrics_stg_df.join(history_day_df, ( \\\n",
    "            F.builtin(\"DATE\")(daily_city_metrics_stg_df['ORDER_TS']) == history_day_df['DATE_VALID_STD']) \\\n",
    "            & (location_df['ISO_COUNTRY_CODE'] == history_day_df['COUNTRY']) \\\n",
    "            & (location_df['CITY'] == history_day_df['CITY_NAME']))\n",
    "\n",
    "    # Aggregate the data\n",
    "    daily_city_metrics_stg_df = daily_city_metrics_stg_df \\\n",
    "                            .group_by(\\\n",
    "                                F.col('DATE_VALID_STD'), \\\n",
    "                                F.col('CITY_NAME'), \\\n",
    "                                F.col('ISO_COUNTRY_CODE')) \\\n",
    "                            .agg( \\\n",
    "                                F.sum('PRICE').alias('DAILY_SALES_SUM'), \\\n",
    "                                F.avg('AVG_TEMPERATURE_AIR_2M_F').alias(\"AVG_TEMPERATURE_F\"), \\\n",
    "                                F.avg(\"TOT_PRECIPITATION_IN\").alias(\"AVG_PRECIPITATION_IN\"), \\\n",
    "                            ) \\\n",
    "                            .select(\\\n",
    "                                F.col(\"DATE_VALID_STD\").alias(\"DATE\"), \\\n",
    "                                F.col(\"CITY_NAME\"), \\\n",
    "                                F.col(\"ISO_COUNTRY_CODE\").alias(\"COUNTRY_DESC\"), \\\n",
    "                                F.builtin(\"ZEROIFNULL\")(F.col(\"DAILY_SALES_SUM\")).alias(\"DAILY_SALES\"), \\\n",
    "                                F.round(F.col(\"AVG_TEMPERATURE_F\"), 2).alias(\"AVG_TEMPERATURE_FAHRENHEIT\"), \\\n",
    "                                F.round(F.col(\"AVG_PRECIPITATION_IN\"), 2).alias(\"AVG_PRECIPITATION_INCHES\"), \\\n",
    "                            )\n",
    "\n",
    "    # Check if table exists\n",
    "    exists = session.sql(f\"SELECT EXISTS ( \\\n",
    "                        SELECT * \\\n",
    "                        FROM INFORMATION_SCHEMA.TABLES \\\n",
    "                        WHERE TABLE_SCHEMA = '{schema}' \\\n",
    "                        AND TABLE_NAME = '{table}') \\\n",
    "                        AS TABLE_EXISTS\").collect()[0]['TABLE_EXISTS']\n",
    "    \n",
    "    # Upsert data\n",
    "    if not exists:\n",
    "        daily_city_metrics_stg_df.write.mode(\"overwrite\").save_as_table(table)\n",
    "    else:\n",
    "\n",
    "        cols_to_update = {c: daily_city_metrics_stg_df[c] for c in daily_city_metrics_stg_df.schema.names}\n",
    "        metadata_col_to_update = {\"META_UPDATED_AT\": F.current_timestamp()}\n",
    "        records = {**cols_to_update, **metadata_col_to_update}\n",
    "        daily_city_metrics_df = session.table('ANALYTICS.DAILY_CITY_METRICS')\n",
    "        daily_city_metrics_df.merge(daily_city_metrics_stg_df, \\\n",
    "                                    (daily_city_metrics_df['DATE'] == daily_city_metrics_stg_df['DATE']) \\\n",
    "                                    & (daily_city_metrics_df['CITY_NAME'] == daily_city_metrics_stg_df['CITY_NAME']) \\\n",
    "                                    & (daily_city_metrics_df['COUNTRY_DESC'] == daily_city_metrics_stg_df['COUNTRY_DESC']), \\\n",
    "                                    [F.when_matched().update(records), F.when_not_matched().insert(records)])\n",
    "\n",
    "    # Return warehouse to original size\n",
    "    session.sql(\"ALTER WAREHOUSE TEST_WH SET WAREHOUSE_SIZE = XSMALL\").collect()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9170ddb3-bde5-4734-ba97-b8d9d0e88317",
   "metadata": {
    "language": "python",
    "name": "cell10",
    "vscode": {
     "languageId": "snowflake-sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Create DAILY_CITY_METRICS_UPSERT_SP\n",
    "CREATE OR REPLACE PROCEDURE DAILY_CITY_METRICS_UPSERT_SP()\n",
    "    RETURNS string\n",
    "    LANGUAGE PYTHON\n",
    "    RUNTIME_VERSION=3.9\n",
    "    PACKAGES=('snowflake-snowpark-python','toml')\n",
    "    HANDLER = 'main'\n",
    "    AS $$\n",
    "        {{daily_city_metrics_upsert}}\n",
    "    $$;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed92fb0-83ce-4769-a1ff-5d3c48fb7c80",
   "metadata": {
    "language": "python",
    "name": "cell11"
   },
   "outputs": [],
   "source": [
    "\n",
    "warehouse_name = 'TEST_WH'\n",
    "database_name = 'data_engineering_with_snowflake_notebooks'\n",
    "schema_name = env\n",
    "schema = root.databases[database_name].schemas[schema_name]\n",
    "dag_op = DAGOperation(schema)\n",
    "\n",
    "\n",
    "# Create the DAG\n",
    "dag_obj = DAG(\n",
    "    name = 'DAG',\n",
    "    schedule = timedelta(days=1), \n",
    "    warehouse = warehouse_name\n",
    ")\n",
    "\n",
    "# set load_excel_files params\n",
    "tables = ['order_detail', 'location']\n",
    "task_params = []\n",
    "for tname in tables:\n",
    "    params = {\n",
    "        'source_schema': 'INTEGRATIONS',\n",
    "        'source_table': 'FROSTBYTE_RAW_STAGE',\n",
    "        'source_directory': 'intro',\n",
    "        'source_file': f\"{tname}.xlsx\",\n",
    "        'source_worksheet': tname,\n",
    "\n",
    "        'target_schema': 'INTEGRATIONS',\n",
    "        'target_table': tname,\n",
    "    }\n",
    "    task_params.append(params)\n",
    "\n",
    "# Define the DAG\n",
    "with dag_obj as dag:\n",
    "    order_detail_excel_task= DAGTask(\n",
    "        name = \"order_detail_excel_DAGTASK\", \n",
    "        definition=f'''EXECUTE NOTEBOOK \"{database_name}\".\"{schema_name}\".\"load_excel_files\"()''',\n",
    "        args=[session, task_params[0]],\n",
    "        warehouse=warehouse_name)\n",
    "    location_excel_task= DAGTask(\n",
    "        name=\"location_excel_DAGTASK\", \n",
    "        definition=f'''EXECUTE NOTEBOOK \"{database_name}\".\"{schema_name}\".\"load_excel_files\"()''',\n",
    "        args=[session, task_params[1]],\n",
    "        warehouse=warehouse_name)\n",
    "    dailty_city_metrics_upsert_sp_dagtask = DAGTask(\n",
    "        name=\"dailty_city_metrics_upsert_sp_dagtask\", \n",
    "        definition=f'''CALL \"{database_name}\".\"{schema_name}\".\"DAILY_CITY_METRICS_UPSERT_SP\"()''', \n",
    "        args=[session, schema_name],\n",
    "        warehouse=warehouse_name)\n",
    "\n",
    "    # Define task precedence\n",
    "    order_detail_excel_task >> dailty_city_metrics_upsert_sp_dagtask\n",
    "    location_excel_task >> dailty_city_metrics_upsert_sp_dagtask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a222e651-e1cf-4243-9cc7-7a82ae6b41cb",
   "metadata": {
    "language": "python",
    "name": "cell12"
   },
   "outputs": [],
   "source": [
    "# Deploy the DAG in Snowflake\n",
    "dag_op.deploy(dag, mode=\"orreplace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d1ad77-f7f9-4ac9-b54b-876f05c2d138",
   "metadata": {
    "language": "python",
    "name": "cell13"
   },
   "outputs": [],
   "source": [
    "# View all dags for schema\n",
    "dag_iter = dag_op.iter_dags()\n",
    "for dag_name in dag_iter:\n",
    "    print(dag_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407b75cb-0b38-4bee-9173-2a6be678d5ce",
   "metadata": {
    "language": "python",
    "name": "cell14"
   },
   "outputs": [],
   "source": [
    "# Trigger dag to run once\n",
    "dag_op.run(dag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4299d6-a26d-4815-a826-fb3070cb2b79",
   "metadata": {
    "language": "python",
    "name": "cell15"
   },
   "outputs": [],
   "source": [
    "# View completed runs past hour\n",
    "dag_op.get_complete_dag_runs(dag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469cf601-9642-4cef-b5f2-2f80b2c56392",
   "metadata": {
    "language": "python",
    "name": "cell16"
   },
   "outputs": [],
   "source": [
    "# View current runs and next scheduled\n",
    "dag_op.get_current_dag_runs(dag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807363c4-d207-4c20-90bf-e706aebf15ce",
   "metadata": {
    "language": "python",
    "name": "cell17"
   },
   "outputs": [],
   "source": [
    "# View all DAG in Snowflake\n",
    "dag_run = DAGRun()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c12b846-47bb-4383-80ef-bb1319dd163d",
   "metadata": {
    "language": "python",
    "name": "cell18"
   },
   "outputs": [],
   "source": [
    "# Drop the DAG in Snowflake\n",
    "dag_op.drop(dag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44e79a8-b2e7-4934-a60b-b977c02b523f",
   "metadata": {
    "language": "python",
    "name": "cell19",
    "vscode": {
     "languageId": "snowflake-sql"
    }
   },
   "outputs": [],
   "source": [
    "-- View Logs\n",
    "\n",
    "SELECT TOP 100 *\n",
    "--   RECORD['severity_text'] AS SEVERITY,\n",
    "--   VALUE AS MESSAGE\n",
    "FROM\n",
    "  data_engineering_with_snowflake_notebooks.INTEGRATIONS.EVENTS\n",
    "-- WHERE 1 = 1\n",
    "--   AND SCOPE['name'] = 'demo_logger'\n",
    "--   AND RECORD_TYPE = 'LOG';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24fae0c-8b15-44ce-ad25-ae0042575be7",
   "metadata": {
    "language": "sql",
    "name": "cell20",
    "vscode": {
     "languageId": "snowflake-sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Teardown\n",
    "\n",
    "DROP DATABASE data_engineering_with_snowflake_notebooks;\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "lastEditStatus": {
   "authorEmail": "ncarlson@mhnchicago.org",
   "authorId": "4427789458918",
   "authorName": "NCARLSON",
   "lastEditTime": 1750989405996,
   "notebookId": "amxfbe6fdiaisexsfuww",
   "sessionId": "023870cc-0966-4774-b3e5-5b865890ae2f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
